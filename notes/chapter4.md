- A "parameter" is the single cell of a weight matrix or bias vector. For this reason, the number of parameters of a linear layer with input dimension `d_in` and output dimension `d_out` is `d_in * d_out` 
- The weights of GPT 2 were made public by OpenAI
- GPT 3 has more or less the same architecture of GPT 2, but with 175 billion parameters (instead of 1.5 billion) and it is trained on a much larger dataset
- Weights of GPT 3 are not public
- Training and inference is cheap on GPT 2, but very expensive on GPT 3 (it requires a cluster of GPUs and a lot of time)
- Recap of GPT-2:
  - **vocab-size**: 50257 (number of tokens generated by the BPE tokenizer)
  - **context-length**: 1024 (maximum number of tokens processed as input via positional embeddings). Recall that positional embeddings do not map token ids but rather positions to a certain vector (like what vocab-size does for token ids)
  - **emb-dim**:768 (dimension of each token embedding, that is the number of columns of the embedding matrix. In other words, each token-id is mapped to a vector of 768 elements)
  - **num-heads**: 12 (number of heads used in the multi-attention mechanism)
  - **num-layers**: 12 (number of trasnformer blocks in the model, each wrapping a multi-headed attention module)
  - **drop-rate**: 0.1 (dropout rate used in the dropouto of causal attention)
  - **bias**: false (whether to use bias vectors in the weight matrices query, key, value of each head)

- Typically is preferred GeLu over ReLu as activation function due to its smoothness (even if GeLu is not convex). Moreover, GeLu leaks some tiny negative values which avoid total waste of information.
- Feedforward layers internally expand vector embeddings into a way bigger dimension: this higher level representation is then evaluated by an activation function, and then data are brought back to the original dimension. Is like extracting and watching the atoms, and then recompose it with slightly different transformation.
- **Shortcut connections**: to mitigate vanishing gradient issues: inputs are added to output every time that embeddings are evaluated by a layer

- A vanilla transformer block is made of 
  - A **layer norm** which computes the z-scores of each vector embeding and maps them to gaussian values according to (learnable) shift and scale
  - A **Multi-headed causal attention layer** evaluating the gaussian vector embeddings 
    - This part of the transformer block is responsible of **identifying and analyzing relationship in the input data**
    - NOtice that no head iin the multi-attention layer involves activation function
  - A **dropout layer**
  - Another **layer norm**
  - A **FeedForward** layer which internally
    - Uses a Linear layer to map vector embeddings into super high dimensional data
    - This high dimensional data is evaluated by GeLu (or other) activation function
    - Another Linear layer maps back the output in the original shape of vector embeddings
    - The role of the **FeedForward** layer is to **modify data individually in each position** 
  - Notice that the original vector embeddings are added to the intermediate result after the first dropout layer (after that we surpassed the multi-head attention module). Likewise, the input of the sacond layer norm is added to the final output (after the second dropout, after the FeedForward). This shortuct connection machanism mitigates the vanishing gradient problem
  - The output of a transformer block is the same of the input. However the input has been "trasnformed" indeed via the contextual information encoded via the multi-head attention modules and feedforward modules

